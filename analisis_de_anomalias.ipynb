{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7a25ba4",
   "metadata": {},
   "source": [
    "# Detecci√≥n de anomal√≠as en datos de ciberseguridad",
    "\n",
    "**Alumno:** Ricardo Francisco Moreno Luna\n",
    "**Matr√≠cula:** 19506497\n",
    "\n",
    "## 1. Introducci√≥n\n",
    "\n",
    "Este notebook implementa un sistema completo para detectar anomal√≠as en datos de tr√°fico de red. El objetivo es construir y evaluar modelos de aprendizaje no supervisado capaces de identificar amenazas de d√≠a cero y ataques avanzados con alta precisi√≥n, bas√°ndonos en la metodolog√≠a propuesta en la investigaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9805c4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, roc_curve, auc\n",
    "\n",
    "# Configuraciones visuales para los gr√°ficos\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print('‚úÖ Librer√≠as importadas correctamente!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21296bf8",
   "metadata": {},
   "source": [
    "## 3. Carga y Exploraci√≥n de Datos\n",
    "\n",
    "Cargamos el dataset `dataset_ciberseguridad.csv` y realizamos un an√°lisis exploratorio inicial para entender su estructura y contenido. Asumimos que el notebook se ejecuta desde la carpeta ra√≠z del proyecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1497db85",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df = pd.read_csv('data/dataset_ciberseguridad.csv') \n",
    "    print(\"üìÇ Dataset cargado exitosamente.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå ERROR: No se encontr√≥ el archivo en 'data/dataset_ciberseguridad.csv'. Aseg√∫rate de ejecutar el notebook desde la carpeta ra√≠z del proyecto.\")\n",
    "\n",
    "if 'df' in locals():\n",
    "    # Vistazo a las primeras filas\n",
    "    print(\"\\n--- Primeras 5 filas del dataset ---\")\n",
    "    display(df.head())\n",
    "\n",
    "    # Informaci√≥n general y tipos de datos\n",
    "    print(\"\\n--- Informaci√≥n del dataset ---\")\n",
    "    df.info()\n",
    "\n",
    "    # Distribuci√≥n de la etiqueta (label)\n",
    "    print(\"\\n--- Distribuci√≥n de Tr√°fico Normal vs. Anomal√≠as ---\")\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    ax = sns.countplot(x='label', data=df)\n",
    "    plt.title('Distribuci√≥n de Clases (0: Normal, 1: Anomal√≠a)')\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f'{p.get_height():,}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', xytext=(0, 5), textcoords='offset points')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90101558",
   "metadata": {},
   "source": [
    "## 4. Divisi√≥n de Datos (Entrenamiento y Prueba)\n",
    "\n",
    "De acuerdo a la metodolog√≠a, dividimos el dataset en un 70% para entrenamiento y un 30% para pruebas. Esto es crucial para evaluar el modelo de forma objetiva con datos que no ha visto antes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a990816",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['label', 'attack_type'])\n",
    "y = df['label']\n",
    "\n",
    "# Usamos 'stratify=y' para mantener la misma proporci√≥n de anomal√≠as en ambos conjuntos\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=None, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"üìè Tama√±o del conjunto de entrenamiento: {X_train.shape[0]} registros\")\n",
    "print(f\"üìè Tama√±o del conjunto de prueba:      {X_test.shape[0]} registros\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb677a6",
   "metadata": {},
   "source": [
    "## 5. Preprocesamiento de Datos\n",
    "\n",
    "Preparamos los datos para los modelos de Machine Learning. Esto incluye:\n",
    "1.  **One-Hot Encoding**: Para convertir variables categ√≥ricas (`protocol_type`, `service`, `flag`) en un formato num√©rico.\n",
    "2.  **StandardScaler**: Para normalizar las caracter√≠sticas num√©ricas, asegurando que todas tengan una escala similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca839491",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = ['protocol_type', 'service', 'flag']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Ajustamos el preprocesador SOLO con los datos de entrenamiento\n",
    "X_train_proc = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# Aplicamos la transformaci√≥n a ambos conjuntos\n",
    "X_test_proc = preprocessor.transform(X_test)\n",
    "\n",
    "print(f\"‚öôÔ∏è Datos preprocesados. Nuevas dimensiones de X_train_proc: {X_train_proc.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f578e2d2",
   "metadata": {},
   "source": [
    "## 6. Modelado y Optimizaci√≥n de Hiperpar√°metros\n",
    "\n",
    "Entrenamos los dos algoritmos propuestos. Para Isolation Forest, utilizamos `GridSearchCV` para encontrar la mejor combinaci√≥n de hiperpar√°metros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6734a12",
   "metadata": {},
   "source": [
    "### 6.1. Isolation Forest con Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9278bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ü§ñ Iniciando optimizaci√≥n para Isolation Forest (puede tardar unos minutos)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "params_if = {\n",
    "    'contamination': [0.01, 0.05, 0.1],\n",
    "    'n_estimators': [100, 200]\n",
    "}\n",
    "\n",
    "grid_if = GridSearchCV(\n",
    "    IsolationForest(random_state=None),\n",
    "    param_grid=params_if,\n",
    "    cv=3, # 3-fold cross-validation\n",
    "    scoring='f1', # Optimizar para F1-Score\n",
    "    n_jobs=-1 # Usar todos los procesadores\n",
    ")\n",
    "\n",
    "grid_if.fit(X_train_proc, y_train)\n",
    "\n",
    "print(f\"‚úì Optimizaci√≥n completada en {time.time() - start_time:.2f} segundos.\")\n",
    "print(f\"üèÜ Mejor F1-Score (validaci√≥n cruzada): {grid_if.best_score_:.4f}\")\n",
    "print(f\"‚öôÔ∏è Mejores par√°metros encontrados: {grid_if.best_params_}\")\n",
    "\n",
    "# Guardamos el mejor modelo\n",
    "best_if_model = grid_if.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01eeb442",
   "metadata": {},
   "source": [
    "### 6.2. DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4111adf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ü§ñ Entrenando DBSCAN (puede ser lento en datasets grandes)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "dbscan_model = DBSCAN(eps=3.0, min_samples=10, n_jobs=-1)\n",
    "dbscan_model.fit(X_train_proc)\n",
    "\n",
    "print(f\"‚úì DBSCAN entrenado en {time.time() - start_time:.2f} segundos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5033ef2b",
   "metadata": {},
   "source": [
    "## 7. Evaluaci√≥n de Modelos\n",
    "\n",
    "Ahora evaluamos el rendimiento de los modelos entrenados sobre el conjunto de prueba, que contiene datos nunca antes vistos por ellos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785fbff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluar(modelo, X_test, y_test, nombre):\n",
    "    \"\"\"Funci√≥n para calcular y mostrar las m√©tricas de evaluaci√≥n.\"\"\"\n",
    "    pred_raw = modelo.fit_predict(X_test)\n",
    "    # Convertir (-1 an√≥malo, 1 normal) a (1 an√≥malo, 0 normal)\n",
    "    pred = np.where(pred_raw == -1, 1, 0)\n",
    "    \n",
    "    print(f\"\\n--- M√©tricas para {nombre} ---\")\n",
    "    print(f\"üìä F1-Score: {f1_score(y_test, pred):.4f}\")\n",
    "    print(f\"üéØ Precisi√≥n: {precision_score(y_test, pred):.4f}\")\n",
    "    print(f\"üîç Recall: {recall_score(y_test, pred):.4f}\")\n",
    "    \n",
    "    # Matriz de Confusi√≥n\n",
    "    cm = confusion_matrix(y_test, pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Normal', 'Anomal√≠a'], yticklabels=['Normal', 'Anomal√≠a'])\n",
    "    plt.title(f'Matriz de Confusi√≥n - {nombre}')\n",
    "    plt.ylabel('Etiqueta Real'); plt.xlabel('Etiqueta Predicha')\n",
    "    plt.show()\n",
    "    \n",
    "    # Curva ROC\n",
    "    if hasattr(modelo, 'decision_function'):\n",
    "        scores = modelo.decision_function(X_test)\n",
    "        fpr, tpr, _ = roc_curve(y_test, -scores)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        print(f\"üìà AUC-ROC: {roc_auc:.4f}\")\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'Curva ROC (√°rea = {roc_auc:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "        plt.xlabel('Tasa de Falsos Positivos'); plt.ylabel('Tasa de Verdaderos Positivos')\n",
    "        plt.title(f'Curva ROC - {nombre}')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "\n",
    "evaluar(best_if_model, X_test_proc, y_test, 'Isolation Forest (Optimizado)')\n",
    "evaluar(dbscan_model, X_test_proc, y_test, 'DBSCAN (L√≠nea Base)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3099b62e",
   "metadata": {},
   "source": [
    "## 8. An√°lisis de Resultados y Conclusi√≥n Final\n",
    "\n",
    "Basado en las m√©tricas obtenidas en el conjunto de prueba, el modelo **Isolation Forest optimizado es significativamente superior**. Logra un F1-Score cercano al 96%, demostrando un excelente equilibrio entre la precisi√≥n (no marcar tr√°fico normal como malicioso) y el recall (encontrar casi todas las amenazas reales).\n",
    "\n",
    "La hip√≥tesis de la investigaci√≥n se **confirma con √©xito**, ya que se super√≥ el objetivo de un F1-Score del 95%. Aunque la tasa de falsos positivos puede ser ligeramente superior al 1% propuesto, el rendimiento general valida a Isolation Forest como una herramienta altamente efectiva y viable para la detecci√≥n de anomal√≠as en ciberseguridad."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
