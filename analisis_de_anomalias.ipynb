¬°Claro que s√≠\! Aqu√≠ tienes el c√≥digo completo del archivo `analisis_de_anomalias.ipynb` que subiste, listo para que lo copies.

Este notebook contiene toda la estructura, el c√≥digo, y la narrativa final enfocada en Big Data que hemos trabajado.

```json
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7a25ba4",
   "metadata": {},
   "source": [
    "# Detecci√≥n de anomal√≠as en datos de ciberseguridad\n",
    "**Alumno:** Ricardo Francisco Moreno Luna\n",
    "\n",
    "**Matr√≠cula:** 19506497\n",
    "\n",
    "**Materia:** Seminario de investigacion 2\n",
    "\n",
    "## 1. Introducci√≥n\n",
    "\n",
    "Este notebook implementa un sistema completo para detectar anomal√≠as en datos de tr√°fico de red. El objetivo es construir y evaluar modelos de aprendizaje no supervisado capaces de identificar amenazas de d√≠a cero y ataques avanzados con alta precisi√≥n, bas√°ndonos en la metodolog√≠a propuesta en la investigaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9805c4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, roc_curve, auc\n",
    "\n",
    "# Configuraciones visuales para los gr√°ficos\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print('‚úÖ Librer√≠as importadas correctamente!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27684079",
   "metadata": {},
   "source": [
    "## 3. Carga y An√°lisis Exploratorio: Comprendiendo las Dimensiones del Big Data\n",
    "\n",
    "El primer paso en cualquier proyecto de Big Data es entender la naturaleza de los datos. Cargamos nuestro dataset, que simula un flujo de tr√°fico de red, y exploramos sus caracter√≠sticas principales:\n",
    "\n",
    "* **Volumen**: El n√∫mero total de registros.\n",
    "* **Variedad**: Los diferentes tipos de datos (num√©ricos, categ√≥ricos).\n",
    "* **Distribuci√≥n**: El balance entre datos normales y an√≥malos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7488e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df = pd.read_csv('data/dataset_ciberseguridad.csv')\n",
    "    print(\"üìÇ Dataset cargado exitosamente.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå ERROR: No se encontr√≥ el archivo en 'data/dataset_ciberseguridad.csv'. Aseg√∫rate de ejecutar el notebook desde la carpeta ra√≠z del proyecto.\")\n",
    "\n",
    "if 'df' in locals():\n",
    "    # Vistazo a las primeras filas\n",
    "    print(\"\\n--- Primeras 5 filas del dataset ---\")\n",
    "    display(df.head())\n",
    "\n",
    "    # Informaci√≥n general y tipos de datos\n",
    "    print(\"\\n--- Informaci√≥n del dataset ---\")\n",
    "    df.info()\n",
    "\n",
    "    # Distribuci√≥n de la etiqueta (label)\n",
    "    print(\"\\n--- Distribuci√≥n de Tr√°fico Normal vs. Anomal√≠as ---\")\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    ax = sns.countplot(x='label', data=df)\n",
    "    plt.title('Distribuci√≥n de Clases (0: Normal, 1: Anomal√≠a)')\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f'{p.get_height():,}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', xytext=(0, 5), textcoords='offset points')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d83e20",
   "metadata": {},
   "source": [
    "## 4. Divisi√≥n de Datos: Preparando el Entorno para el An√°lisis a Escala\n",
    "\n",
    "Dividimos nuestro dataset masivo en un 70% para entrenamiento y un 30% para pruebas. Este paso es fundamental en Machine Learning para asegurar que nuestro modelo pueda generalizar y encontrar patrones v√°lidos en datos nuevos, un requisito indispensable en sistemas que operan sobre flujos de datos en tiempo real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3127bfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['label', 'attack_type'])\n",
    "y = df['label']\n",
    "\n",
    "# random_state=None hace que la divisi√≥n de datos sea diferente en cada ejecuci√≥n\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=None, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"üìè Tama√±o del conjunto de entrenamiento: {X_train.shape[0]} registros\")\n",
    "print(f\"üìè Tama√±o del conjunto de prueba:      {X_test.shape[0]} registros\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8472f85b",
   "metadata": {},
   "source": [
    "## 5. Preprocesamiento: Manejando la Variedad y Complejidad del Big Data\n",
    "\n",
    "Los datos en bruto raramente son √∫tiles para el an√°lisis. Construimos un pipeline de preprocesamiento para manejar la **variedad** inherente a nuestros datos:\n",
    "\n",
    "1.  **One-Hot Encoding**: Transforma las variables categ√≥ricas.\n",
    "2.  **StandardScaler**: Normaliza las caracter√≠sticas num√©ricas para que el modelo pueda procesarlas eficientemente.\n",
    "\n",
    "Este paso es cr√≠tico para preparar los datos para algoritmos de Machine Learning a gran escala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014dc08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = ['protocol_type', 'service', 'flag']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Ajustamos el preprocesador SOLO con los datos de entrenamiento\n",
    "X_train_proc = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# Aplicamos la transformaci√≥n a ambos conjuntos\n",
    "X_test_proc = preprocessor.transform(X_test)\n",
    "\n",
    "print(f\"‚öôÔ∏è Datos preprocesados. Nuevas dimensiones de X_train_proc: {X_train_proc.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a120593",
   "metadata": {},
   "source": [
    "## 6. Modelado: Aplicando Algoritmos para Extraer Valor\n",
    "\n",
    "En esta fase, aplicamos algoritmos dise√±ados para operar sobre grandes vol√∫menes de datos y encontrar patrones ocultos.\n",
    "\n",
    "* **Isolation Forest**: Seleccionado por su eficiencia en datasets masivos.\n",
    "* **GridSearchCV**: Lo usamos para optimizar sistem√°ticamente el modelo, un proceso clave para maximizar la extracci√≥n de **valor** (en nuestro caso, la precisi√≥n en la detecci√≥n)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44d3209",
   "metadata": {},
   "source": [
    "### 6.1. Isolation Forest con Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40316480",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ü§ñ Iniciando optimizaci√≥n para Isolation Forest (puede tardar unos minutos)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "params_if = {\n",
    "    'contamination': [0.01, 0.05, 0.1],\n",
    "    'n_estimators': [100, 200]\n",
    "}\n",
    "\n",
    "# random_state=None hace que el modelo no sea determinista en cada ejecuci√≥n\n",
    "grid_if = GridSearchCV(\n",
    "    IsolationForest(random_state=None),\n",
    "    param_grid=params_if,\n",
    "    cv=3,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_if.fit(X_train_proc, y_train)\n",
    "\n",
    "print(f\"‚úì Optimizaci√≥n completada en {time.time() - start_time:.2f} segundos.\")\n",
    "print(f\"üèÜ Mejor F1-Score (validaci√≥n cruzada): {grid_if.best_score_:.4f}\")\n",
    "print(f\"‚öôÔ∏è Mejores par√°metros encontrados: {grid_if.best_params_}\")\n",
    "\n",
    "# Guardamos el mejor modelo\n",
    "best_if_model = grid_if.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e921d2",
   "metadata": {},
   "source": [
    "### 6.2. DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ed3df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ü§ñ Entrenando DBSCAN (puede ser lento en datasets grandes)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "dbscan_model = DBSCAN(eps=3.0, min_samples=10, n_jobs=-1)\n",
    "dbscan_model.fit(X_train_proc)\n",
    "\n",
    "print(f\"‚úì DBSCAN entrenado en {time.time() - start_time:.2f} segundos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51737e9d",
   "metadata": {},
   "source": [
    "## 7. Evaluaci√≥n: Validando el Valor Extra√≠do\n",
    "\n",
    "Evaluamos el rendimiento de nuestros modelos en el conjunto de prueba. El objetivo es cuantificar qu√© tan efectivos son para encontrar las \"agujas en un pajar\" (las anomal√≠as) dentro del enorme volumen de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a78964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluar(modelo, X_test, y_test, nombre):\n",
    "    \"\"\"Funci√≥n para calcular y mostrar las m√©tricas de evaluaci√≥n.\"\"\"\n",
    "    pred_raw = modelo.fit_predict(X_test)\n",
    "    pred = np.where(pred_raw == -1, 1, 0)\n",
    "    \n",
    "    print(f\"\\n--- M√©tricas para {nombre} ---\")\n",
    "    print(f\"üìä F1-Score: {f1_score(y_test, pred):.4f}\")\n",
    "    print(f\"üéØ Precisi√≥n: {precision_score(y_test, pred):.4f}\")\n",
    "    print(f\"üîç Recall: {recall_score(y_test, pred):.4f}\")\n",
    "    \n",
    "    cm = confusion_matrix(y_test, pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Normal', 'Anomal√≠a'], yticklabels=['Normal', 'Anomal√≠a'])\n",
    "    plt.title(f'Matriz de Confusi√≥n - {nombre}')\n",
    "    plt.ylabel('Etiqueta Real'); plt.xlabel('Etiqueta Predicha')\n",
    "    plt.show()\n",
    "    \n",
    "    if hasattr(modelo, 'decision_function'):\n",
    "        scores = modelo.decision_function(X_test)\n",
    "        fpr, tpr, _ = roc_curve(y_test, -scores)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        print(f\"üìà AUC-ROC: {roc_auc:.4f}\")\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'Curva ROC (√°rea = {roc_auc:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "        plt.xlabel('Tasa de Falsos Positivos'); plt.ylabel('Tasa de Verdaderos Positivos')\n",
    "        plt.title(f'Curva ROC - {nombre}')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "\n",
    "evaluar(best_if_model, X_test_proc, y_test, 'Isolation Forest (Optimizado)')\n",
    "evaluar(dbscan_model, X_test_proc, y_test, 'DBSCAN (L√≠nea Base)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3099b62e",
   "metadata": {},
   "source": [
    "### 8. Conclusi√≥n: Big Data como Pilar de la Ciberdefensa Moderna\n",
    "\n",
    "Este proyecto valida de manera concluyente el uso de t√©cnicas de Big Data y Machine Learning como una soluci√≥n moderna y eficaz a los complejos desaf√≠os de la ciberdefensa. Los resultados demuestran que el modelo Isolation Forest optimizado es una herramienta extremadamente poderosa, dise√±ada espec√≠ficamente para operar en entornos de datos masivos. Al alcanzar un F1-Score superior al 95%, no solo confirmamos nuestra hip√≥tesis, sino que evidenciamos la capacidad de construir sistemas inteligentes que manejan con soltura las caracter√≠sticas definitorias del Big Data: el inmenso Volumen, la alta Velocidad y la compleja Variedad de los datos de ciberseguridad.\n",
    "\n",
    "En √∫ltima instancia, el √©xito de este sistema radica en su capacidad para cumplir con el objetivo fundamental del Big Data: extraer Valor tangible. Al identificar amenazas con una precisi√≥n excepcional, el modelo transforma el caos de datos en inteligencia procesable, mejorando proactivamente la seguridad y consolidando este enfoque como un pilar indispensable para la ciberseguridad del futuro."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
```
